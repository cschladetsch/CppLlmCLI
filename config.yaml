# LLM REPL Configuration File

# Default provider (groq, together, ollama)
provider: groq

# API key - can also be set via environment variables
# api_key: ${GROQ_API_KEY}

# Groq Configuration
groq:
  model: llama-3.1-70b-versatile
  temperature: 0.7
  max_tokens: 2048
  api_url: https://api.groq.com/openai/v1

# Together AI Configuration
together:
  model: meta-llama/Llama-2-70b-chat-hf
  temperature: 0.7
  max_tokens: 2048
  api_url: https://api.together.xyz/v1

# Ollama Configuration (Local)
ollama:
  model: llama3.1
  temperature: 0.7
  max_tokens: 2048
  api_url: http://localhost:11434

# REPL Settings
repl:
  history_file: ~/.llm_repl_history
  max_history: 100
  system_prompt: "You are a helpful AI assistant."
  streaming: true
  markdown_rendering: true
  prompt_prefix: "> "
  ai_prefix: "AI: "